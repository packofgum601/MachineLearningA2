{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\"> COSC 2673/2793 | Machine Learning </div>\n",
    "\n",
    "## <div align=\"center\"> Assignment 2 - Joseph Packham (s3838978) and Kylie Nguyen (s3946026) </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This report will cover the process of producing a machine learning model that will predict energy usage...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "# import seaborn package for plotting scatterplots\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import pydot as pyd\n",
    "\n",
    "\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.utils import compute_sample_weight\n",
    "from tensorflow.keras.losses import Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in CSV file and display first 5 rows\n",
    "energyUse_df = pd.read_csv(\"./dataset/UCI-electricity/UCI_data.csv\", delimiter=\",\")\n",
    "energyUse_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "First, the data is investigated through EDA. It is observed that the shape of the dataframe is 19735 rows of data with 28 columns, where 1 column is the target variable (energy usage in Wh), and the remaining columns are the attributes. According to the description of the data, these attributes cover the temperature and humidity of different rooms in the house, as well as outside, along with a few other weather related variables such as pressure and windspeed. It is noted that there are two variables listed as \"Random Variable\". Using the .info() function, it is confirmed that there are no null values within the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for any null values, using shape to compare\n",
    "print(\"Shape of Energy Use dataframe: \", energyUse_df.shape, \"\\n\")\n",
    "\n",
    "energyUse_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the describe function, the count, mean, standard deviation, quantiles and the minimum and maximum values of the data are returned. With these values it is seen that, although the range of the values among the variables regarding humidity and temperature are relatively similar, there are cases where the range differs greatly. For example, the range of Windspeed is between 0-14, whereas the range of target energy is between 10-1110. This suggests that feature scaling should be done later in the process, as the differing ranges may cause problems or confuse the learning algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energyUse_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data distribution\n",
    "\n",
    "In order to observe the distributions of each variable, histograms are plotted for the variables other than date, as the date variable is of type object and cannot be plotted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of columns other than date\n",
    "columns = (energyUse_df.columns).difference([\"date\"])\n",
    "# plot histogram for all variables other than date\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, column in enumerate(columns):\n",
    "    plt.subplot(6, 5, i + 1)\n",
    "    plt.hist(energyUse_df[column], alpha=0.3, color=\"b\", density=True)\n",
    "    plt.title(column)\n",
    "    plt.xticks(rotation=\"vertical\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations:**\n",
    ">\n",
    "> - There are a number of attributes that appear to be skewed, eg. RH_5, RH_Out, T2 etc.\n",
    "> - The two random variables are very evenly distributed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display boxplot for the target, energy usage, variable\n",
    "plt.boxplot(energyUse_df[\"TARGET_energy\"])\n",
    "plt.title(\"Energy Usage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After displaying the boxplot for the target variable, it is observed that there are a number of outliers above the lower limit. These values will be dropped as to prevent these dramatically different values from affecting the model. The outliers are dropped using the IQR method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the quantiles and IQR\n",
    "q1 = energyUse_df[\"TARGET_energy\"].quantile(0.25)\n",
    "q3 = energyUse_df[\"TARGET_energy\"].quantile(0.75)\n",
    "IQR = q3 - q1\n",
    "\n",
    "# calculate lower and upper limits\n",
    "lowerLimit = q1 - (1.5 * IQR)\n",
    "upperLimit = q3 + (1.5 * IQR)\n",
    "\n",
    "# get rid of rows with outliers from the dataframe\n",
    "energyUse_df = energyUse_df.loc[\n",
    "    (energyUse_df[\"TARGET_energy\"] > lowerLimit)\n",
    "    & (energyUse_df[\"TARGET_energy\"] < upperLimit)\n",
    "]\n",
    "\n",
    "# display boxplot without outliers\n",
    "plt.boxplot(energyUse_df[\"TARGET_energy\"])\n",
    "plt.title(\"Energy Usage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energyUse_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship between variables\n",
    "\n",
    "Using scatterplots, the relationship between the target variable, Energy Usage, against the other attributes in the dataframe is explored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plot scatterplots for all features against target variable\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, column in enumerate(columns):\n",
    "    plt.subplot(6, 5, i + 1)\n",
    "    sns.scatterplot(data=energyUse_df, x=column, y=\"TARGET_energy\")\n",
    "    plt.title(column)\n",
    "\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get list of columns other than date and target\n",
    "# columns = (energyUse_df.columns).difference([\"date\", \"TARGET_energy\"])\n",
    "\n",
    "# g = sns.PairGrid(data=energyUse_df, vars=columns, hue=\"TARGET_energy\")\n",
    "# g.map(sns.scatterplot)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations:**\n",
    ">\n",
    "> - There are some plots that show that a linear decision boundary may be able to separate the two classes. eg.\n",
    "> - Whereas there are some plots that show that a non-linear decision boundary may be to separate the two classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get df without date column\n",
    "energyUse_df_noDate = energyUse_df.drop(columns=[\"date\"])\n",
    "\n",
    "# plot correlation plot\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "corr = energyUse_df_noDate.corr()\n",
    "ax = sns.heatmap(\n",
    "    corr,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True,\n",
    ")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, horizontalalignment=\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations:**\n",
    ">\n",
    "> - Variables relating to temperature are highly positively correlated with each other, and variables that are related to humidity are similarly, highly positively correlated with each other.\n",
    "> - Variables involving temperature generally have either a slight positive, or slight negative correlation with variables involving humidity.\n",
    "> - RH_6, the humidity outside the building (northside) seems to be quite negatively correlated with variables regarding temperature.\n",
    "> - The two random variables do not seem to be correlated with any other variable other being highly correlated with themselves as well as each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energyUse_df[\"TARGET_energy\"].hist(figsize=(5, 5))\n",
    "plt.xlabel(\"Energy Usage\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Neural Network - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the dataset into 70% train and 15% test and 15% val\n",
    "with pd.option_context(\"mode.chained_assignment\", None):\n",
    "    LR_train, LR_test = train_test_split(\n",
    "        energyUse_df, test_size=0.3, shuffle=True, random_state=42\n",
    "    )\n",
    "    LR_test, LR_val = train_test_split(\n",
    "        LR_test, test_size=0.5, shuffle=True, random_state=42\n",
    "    )\n",
    "\n",
    "# Separate the target and the attributes\n",
    "LR_X_train = LR_train.drop([\"TARGET_energy\", \"date\"], axis=1)\n",
    "LR_y_train = LR_train[\"TARGET_energy\"]\n",
    "\n",
    "LR_X_test = LR_test.drop([\"TARGET_energy\", \"date\"], axis=1)\n",
    "LR_y_test = LR_test[\"TARGET_energy\"]\n",
    "\n",
    "LR_X_val = LR_val.drop([\"TARGET_energy\", \"date\"], axis=1)\n",
    "LR_y_val = LR_val[\"TARGET_energy\"]\n",
    "\n",
    "print(\"LR_X_train shape: \", LR_X_train.shape)\n",
    "print(\"LR_y_train shape: \", LR_y_train.shape)\n",
    "print(\"LR_X_test shape: \", LR_X_test.shape)\n",
    "print(\"LR_y_test shape: \", LR_y_test.shape)\n",
    "print(\"LR_X_val shape: \", LR_X_val.shape)\n",
    "print(\"LR_y_val shape: \", LR_y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energyUse_df_X = energyUse_df.drop([\"TARGET_energy\", \"date\"], axis=1)\n",
    "\n",
    "# plotting histograms of both training and test datasets\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, col in enumerate(energyUse_df_X.columns):\n",
    "    plt.subplot(6, 5, i + 1)\n",
    "    plt.hist(LR_X_train[col], alpha=0.3, color=\"b\", density=True)\n",
    "    plt.hist(LR_X_test[col], alpha=0.3, color=\"r\", density=True)\n",
    "    plt.title(col)\n",
    "    plt.xticks(rotation=\"vertical\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model, Unscaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "\n",
    "# unscaled\n",
    "model_us_lr = LinearRegression().fit(LR_X_train, LR_y_train)\n",
    "LR_y_val_pred_US = model_us_lr.predict(LR_X_val)\n",
    "\n",
    "r2_us_lr = r2_score(LR_y_val, LR_y_val_pred_US)\n",
    "print(\n",
    "    \"The R^2 score for the linear regression model (without feature scaling) is: {:.3f}\".format(\n",
    "        r2_us_lr\n",
    "    )\n",
    ")\n",
    "\n",
    "MSE_us_lr = np.square(np.subtract(LR_y_val, LR_y_val_pred_US)).mean()\n",
    "RMSE_us_lr = math.sqrt(MSE_us_lr)\n",
    "\n",
    "print(\n",
    "    \"The RMSE score for the linear regression model (without feature scaling) is: {:.3f}\".format(\n",
    "        RMSE_us_lr\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting using linear model and plotting predicted vs actual values\n",
    "\n",
    "fig, energyUse_LinearRegression = plt.subplots()\n",
    "energyUse_LinearRegression.scatter(\n",
    "    LR_y_val, LR_y_val_pred_US, s=25, cmap=plt.cm.coolwarm, zorder=10\n",
    ")\n",
    "\n",
    "lims = [\n",
    "    np.min(\n",
    "        [energyUse_LinearRegression.get_xlim(), energyUse_LinearRegression.get_ylim()]\n",
    "    ),\n",
    "    np.max(\n",
    "        [energyUse_LinearRegression.get_xlim(), energyUse_LinearRegression.get_ylim()]\n",
    "    ),\n",
    "]\n",
    "\n",
    "energyUse_LinearRegression.plot(lims, lims, \"k--\", alpha=0.75, zorder=0)\n",
    "energyUse_LinearRegression.plot(\n",
    "    lims,\n",
    "    [\n",
    "        np.mean(LR_y_train),\n",
    "    ]\n",
    "    * 2,\n",
    "    \"r--\",\n",
    "    alpha=0.75,\n",
    "    zorder=0,\n",
    ")\n",
    "energyUse_LinearRegression.set_aspect(\"equal\")\n",
    "energyUse_LinearRegression.set_xlim(lims)\n",
    "energyUse_LinearRegression.set_ylim(lims)\n",
    "\n",
    "plt.xlabel(\"Actual Energy Use\")\n",
    "plt.ylabel(\"Predicted Energy Use\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residuals for unscaled\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(LR_y_val, LR_y_val - LR_y_val_pred_US, s=25, cmap=plt.cm.coolwarm, zorder=10)\n",
    "\n",
    "xlims = ax.get_xlim()\n",
    "ax.plot(\n",
    "    xlims,\n",
    "    [\n",
    "        0.0,\n",
    "    ]\n",
    "    * 2,\n",
    "    \"k--\",\n",
    "    alpha=0.75,\n",
    "    zorder=0,\n",
    ")\n",
    "ax.set_xlim(xlims)\n",
    "\n",
    "plt.xlabel(\"Actual Energy Use\")\n",
    "plt.ylabel(\"Residual\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with MinMaxScaling and Power Transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling all features, normalising skewed features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "logNorm_attributes = [\n",
    "    \"RH_1\",\n",
    "    \"T2\",\n",
    "    \"T3\",\n",
    "    \"RH_3\",\n",
    "    \"RH_4\",\n",
    "    \"T5\",\n",
    "    \"RH_5\",\n",
    "    \"T6\",\n",
    "    \"RH_6\",\n",
    "    \"T7\",\n",
    "    \"RH_7\",\n",
    "    \"RH_8\",\n",
    "    \"T9\",\n",
    "    \"RH_9\",\n",
    "    \"T_out\",\n",
    "    \"Press_mm_hg\",\n",
    "    \"RH_out\",\n",
    "    \"Windspeed\",\n",
    "    \"Visibility\",\n",
    "]\n",
    "minmax_attributes = list(\n",
    "    set(energyUse_df_X.columns).difference(set(logNorm_attributes))\n",
    ")\n",
    "\n",
    "LR_X_train_scaled = LR_X_train.copy()\n",
    "LR_X_val_scaled = LR_X_val.copy()\n",
    "\n",
    "minmaxscaler = MinMaxScaler().fit(LR_X_train_scaled.loc[:, minmax_attributes])\n",
    "LR_X_train_scaled.loc[:, minmax_attributes] = minmaxscaler.transform(\n",
    "    LR_X_train_scaled.loc[:, minmax_attributes]\n",
    ")\n",
    "LR_X_val_scaled.loc[:, minmax_attributes] = minmaxscaler.transform(\n",
    "    LR_X_val_scaled.loc[:, minmax_attributes]\n",
    ")\n",
    "\n",
    "powertransformer = PowerTransformer(method=\"yeo-johnson\", standardize=False).fit(\n",
    "    LR_X_train.loc[:, logNorm_attributes]\n",
    ")\n",
    "LR_X_train_scaled.loc[:, logNorm_attributes] = powertransformer.transform(\n",
    "    LR_X_train.loc[:, logNorm_attributes]\n",
    ")\n",
    "LR_X_val_scaled.loc[:, logNorm_attributes] = powertransformer.transform(\n",
    "    LR_X_val.loc[:, logNorm_attributes]\n",
    ")\n",
    "\n",
    "minmaxscaler_pt = MinMaxScaler().fit(LR_X_train_scaled.loc[:, logNorm_attributes])\n",
    "LR_X_train_scaled.loc[:, logNorm_attributes] = minmaxscaler_pt.transform(\n",
    "    LR_X_train_scaled.loc[:, logNorm_attributes]\n",
    ")\n",
    "LR_X_val_scaled.loc[:, logNorm_attributes] = minmaxscaler_pt.transform(\n",
    "    LR_X_val_scaled.loc[:, logNorm_attributes]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all histograms after scaling and normalisation\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, col in enumerate(LR_X_train_scaled.columns):\n",
    "    plt.subplot(6, 5, i + 1)\n",
    "    plt.hist(LR_X_train_scaled[col], alpha=0.3, color=\"b\", density=True)\n",
    "    plt.hist(LR_X_val_scaled[col], alpha=0.3, color=\"r\", density=True)\n",
    "    plt.title(col)\n",
    "    plt.xticks(rotation=\"vertical\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting a linear regression model\n",
    "model_scaled_lr = LinearRegression().fit(LR_X_train_scaled, LR_y_train)\n",
    "\n",
    "# predicting using linear model and plotting predicted vs actual values\n",
    "LR_y_val_pred_scaled = model_scaled_lr.predict(LR_X_val_scaled)\n",
    "\n",
    "fig, energyUse_LinearRegression = plt.subplots()\n",
    "energyUse_LinearRegression.scatter(\n",
    "    LR_y_val, LR_y_val_pred_scaled, s=25, cmap=plt.cm.coolwarm, zorder=10\n",
    ")\n",
    "\n",
    "lims = [\n",
    "    np.min(\n",
    "        [energyUse_LinearRegression.get_xlim(), energyUse_LinearRegression.get_ylim()]\n",
    "    ),\n",
    "    np.max(\n",
    "        [energyUse_LinearRegression.get_xlim(), energyUse_LinearRegression.get_ylim()]\n",
    "    ),\n",
    "]\n",
    "\n",
    "energyUse_LinearRegression.plot(lims, lims, \"k--\", alpha=0.75, zorder=0)\n",
    "energyUse_LinearRegression.plot(\n",
    "    lims,\n",
    "    [\n",
    "        np.mean(LR_y_train),\n",
    "    ]\n",
    "    * 2,\n",
    "    \"r--\",\n",
    "    alpha=0.75,\n",
    "    zorder=0,\n",
    ")\n",
    "energyUse_LinearRegression.set_aspect(\"equal\")\n",
    "energyUse_LinearRegression.set_xlim(lims)\n",
    "energyUse_LinearRegression.set_ylim(lims)\n",
    "\n",
    "plt.xlabel(\"Actual Energy Use\")\n",
    "plt.ylabel(\"Predicted Energy Use\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residuals for scaled\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(\n",
    "    LR_y_val, LR_y_val - LR_y_val_pred_scaled, s=25, cmap=plt.cm.coolwarm, zorder=10\n",
    ")\n",
    "\n",
    "xlims = ax.get_xlim()\n",
    "ax.plot(\n",
    "    xlims,\n",
    "    [\n",
    "        0.0,\n",
    "    ]\n",
    "    * 2,\n",
    "    \"k--\",\n",
    "    alpha=0.75,\n",
    "    zorder=0,\n",
    ")\n",
    "ax.set_xlim(xlims)\n",
    "\n",
    "plt.xlabel(\"Actual Energy Use\")\n",
    "plt.ylabel(\"Residual\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled\n",
    "r2_lr_scaled = r2_score(LR_y_val, LR_y_val_pred_scaled)\n",
    "\n",
    "print(\n",
    "    \"The R^2 score for the linear regression model (with feature scaling) is: {:.3f}\".format(\n",
    "        r2_lr_scaled\n",
    "    )\n",
    ")\n",
    "\n",
    "MSE_lr_scaled = np.square(np.subtract(LR_y_val, LR_y_val_pred_scaled)).mean()\n",
    "RMSE_lr_scaled = math.sqrt(MSE_lr_scaled)\n",
    "\n",
    "print(\n",
    "    \"The RMSE score for the linear regression model (with feature scaling) is: {:.3f}\".format(\n",
    "        RMSE_lr_scaled\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day of Week Column + Scaled & Transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to use date to see if that makes model perform better\n",
    "energyUse_df[\"date\"] = pd.to_datetime(energyUse_df[\"date\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "energyUse_df[\"day_of_week\"] = energyUse_df[\"date\"].dt.dayofweek\n",
    "\n",
    "# split the dataset into 70% train and 15% test and 15% val\n",
    "with pd.option_context(\"mode.chained_assignment\", None):\n",
    "    LR_train, LR_test = train_test_split(\n",
    "        energyUse_df, test_size=0.3, shuffle=True, random_state=42\n",
    "    )\n",
    "    LR_test, LR_val = train_test_split(\n",
    "        LR_test, test_size=0.5, shuffle=True, random_state=42\n",
    "    )\n",
    "\n",
    "# Separate the target and the attributes\n",
    "LR_X_train = LR_train.drop([\"TARGET_energy\", \"date\"], axis=1)\n",
    "LR_y_train = LR_train[\"TARGET_energy\"]\n",
    "\n",
    "LR_X_test = LR_test.drop([\"TARGET_energy\", \"date\"], axis=1)\n",
    "LR_y_test = LR_test[\"TARGET_energy\"]\n",
    "\n",
    "LR_X_val = LR_val.drop([\"TARGET_energy\", \"date\"], axis=1)\n",
    "LR_y_val = LR_val[\"TARGET_energy\"]\n",
    "\n",
    "print(\"LR_X_train shape: \", LR_X_train.shape)\n",
    "print(\"LR_y_train shape: \", LR_y_train.shape)\n",
    "print(\"LR_X_test shape: \", LR_X_test.shape)\n",
    "print(\"LR_y_test shape: \", LR_y_test.shape)\n",
    "print(\"LR_X_val shape: \", LR_X_val.shape)\n",
    "print(\"LR_y_val shape: \", LR_y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling all features, normalising skewed features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "logNorm_attributes = [\n",
    "    \"RH_1\",\n",
    "    \"T2\",\n",
    "    \"T3\",\n",
    "    \"RH_3\",\n",
    "    \"RH_4\",\n",
    "    \"T5\",\n",
    "    \"RH_5\",\n",
    "    \"T6\",\n",
    "    \"RH_6\",\n",
    "    \"T7\",\n",
    "    \"RH_7\",\n",
    "    \"RH_8\",\n",
    "    \"T9\",\n",
    "    \"RH_9\",\n",
    "    \"T_out\",\n",
    "    \"Press_mm_hg\",\n",
    "    \"RH_out\",\n",
    "    \"Windspeed\",\n",
    "    \"Visibility\",\n",
    "]\n",
    "minmax_attributes = list(\n",
    "    set(energyUse_df_X.columns).difference(set(logNorm_attributes))\n",
    ")\n",
    "\n",
    "LR_X_train_scaled = LR_X_train.copy()\n",
    "LR_X_val_scaled = LR_X_val.copy()\n",
    "\n",
    "minmaxscaler = MinMaxScaler().fit(LR_X_train_scaled.loc[:, minmax_attributes])\n",
    "LR_X_train_scaled.loc[:, minmax_attributes] = minmaxscaler.transform(\n",
    "    LR_X_train_scaled.loc[:, minmax_attributes]\n",
    ")\n",
    "LR_X_val_scaled.loc[:, minmax_attributes] = minmaxscaler.transform(\n",
    "    LR_X_val_scaled.loc[:, minmax_attributes]\n",
    ")\n",
    "\n",
    "powertransformer = PowerTransformer(method=\"yeo-johnson\", standardize=False).fit(\n",
    "    LR_X_train.loc[:, logNorm_attributes]\n",
    ")\n",
    "LR_X_train_scaled.loc[:, logNorm_attributes] = powertransformer.transform(\n",
    "    LR_X_train.loc[:, logNorm_attributes]\n",
    ")\n",
    "LR_X_val_scaled.loc[:, logNorm_attributes] = powertransformer.transform(\n",
    "    LR_X_val.loc[:, logNorm_attributes]\n",
    ")\n",
    "\n",
    "minmaxscaler_pt = MinMaxScaler().fit(LR_X_train_scaled.loc[:, logNorm_attributes])\n",
    "LR_X_train_scaled.loc[:, logNorm_attributes] = minmaxscaler_pt.transform(\n",
    "    LR_X_train_scaled.loc[:, logNorm_attributes]\n",
    ")\n",
    "LR_X_val_scaled.loc[:, logNorm_attributes] = minmaxscaler_pt.transform(\n",
    "    LR_X_val_scaled.loc[:, logNorm_attributes]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting a linear regression model\n",
    "model_scaled_lr_wDayOfWeek = LinearRegression().fit(LR_X_train_scaled, LR_y_train)\n",
    "\n",
    "# predicting using linear model and plotting predicted vs actual values\n",
    "LR_y_val_pred_dayOfWeek = model_scaled_lr_wDayOfWeek.predict(LR_X_val_scaled)\n",
    "\n",
    "fig, energyUse_wDayOfWeek_LinearRegression = plt.subplots()\n",
    "energyUse_wDayOfWeek_LinearRegression.scatter(\n",
    "    LR_y_val, LR_y_val_pred_dayOfWeek, s=25, cmap=plt.cm.coolwarm, zorder=10\n",
    ")\n",
    "\n",
    "lims = [\n",
    "    np.min(\n",
    "        [\n",
    "            energyUse_wDayOfWeek_LinearRegression.get_xlim(),\n",
    "            energyUse_wDayOfWeek_LinearRegression.get_ylim(),\n",
    "        ]\n",
    "    ),\n",
    "    np.max(\n",
    "        [\n",
    "            energyUse_wDayOfWeek_LinearRegression.get_xlim(),\n",
    "            energyUse_wDayOfWeek_LinearRegression.get_ylim(),\n",
    "        ]\n",
    "    ),\n",
    "]\n",
    "\n",
    "energyUse_wDayOfWeek_LinearRegression.plot(lims, lims, \"k--\", alpha=0.75, zorder=0)\n",
    "energyUse_wDayOfWeek_LinearRegression.plot(\n",
    "    lims,\n",
    "    [\n",
    "        np.mean(LR_y_train),\n",
    "    ]\n",
    "    * 2,\n",
    "    \"r--\",\n",
    "    alpha=0.75,\n",
    "    zorder=0,\n",
    ")\n",
    "energyUse_wDayOfWeek_LinearRegression.set_aspect(\"equal\")\n",
    "energyUse_wDayOfWeek_LinearRegression.set_xlim(lims)\n",
    "energyUse_wDayOfWeek_LinearRegression.set_ylim(lims)\n",
    "\n",
    "plt.xlabel(\"Actual Energy Use\")\n",
    "plt.ylabel(\"Predicted Energy Use\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(\n",
    "    LR_y_val, LR_y_val - LR_y_val_pred_dayOfWeek, s=25, cmap=plt.cm.coolwarm, zorder=10\n",
    ")\n",
    "\n",
    "xlims = ax.get_xlim()\n",
    "ax.plot(\n",
    "    xlims,\n",
    "    [\n",
    "        0.0,\n",
    "    ]\n",
    "    * 2,\n",
    "    \"k--\",\n",
    "    alpha=0.75,\n",
    "    zorder=0,\n",
    ")\n",
    "ax.set_xlim(xlims)\n",
    "\n",
    "plt.xlabel(\"Actual Energy Use\")\n",
    "plt.ylabel(\"Residual\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled + dayOfWeek\n",
    "r2_lr = r2_score(LR_y_val, LR_y_val_pred_dayOfWeek)\n",
    "\n",
    "print(\n",
    "    \"The R^2 score for the linear regression model (with feature scaling + dayOfWeek) is: {:.3f}\".format(\n",
    "        r2_lr\n",
    "    )\n",
    ")\n",
    "\n",
    "MSE_lr = np.square(np.subtract(LR_y_val, LR_y_val_pred_dayOfWeek)).mean()\n",
    "RMSE_lr = math.sqrt(MSE_lr)\n",
    "\n",
    "print(\n",
    "    \"The RMSE score for the linear regression model (with feature scaling + dayOfWeek) is: {:.3f}\".format(\n",
    "        RMSE_lr\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day of Week Column + Unscaled & Untransformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting a linear regression model\n",
    "model_us_lr_wDayOfWeek = LinearRegression().fit(LR_X_train, LR_y_train)\n",
    "\n",
    "# predicting using linear model and plotting predicted vs actual values\n",
    "LR_y_val_pred_dayOfWeek_us = model_us_lr_wDayOfWeek.predict(LR_X_val)\n",
    "\n",
    "fig, energyUse_wDayOfWeek_LinearRegression = plt.subplots()\n",
    "energyUse_wDayOfWeek_LinearRegression.scatter(\n",
    "    LR_y_val, LR_y_val_pred_dayOfWeek_us, s=25, cmap=plt.cm.coolwarm, zorder=10\n",
    ")\n",
    "\n",
    "lims = [\n",
    "    np.min(\n",
    "        [\n",
    "            energyUse_wDayOfWeek_LinearRegression.get_xlim(),\n",
    "            energyUse_wDayOfWeek_LinearRegression.get_ylim(),\n",
    "        ]\n",
    "    ),\n",
    "    np.max(\n",
    "        [\n",
    "            energyUse_wDayOfWeek_LinearRegression.get_xlim(),\n",
    "            energyUse_wDayOfWeek_LinearRegression.get_ylim(),\n",
    "        ]\n",
    "    ),\n",
    "]\n",
    "\n",
    "energyUse_wDayOfWeek_LinearRegression.plot(lims, lims, \"k--\", alpha=0.75, zorder=0)\n",
    "energyUse_wDayOfWeek_LinearRegression.plot(\n",
    "    lims,\n",
    "    [\n",
    "        np.mean(LR_y_train),\n",
    "    ]\n",
    "    * 2,\n",
    "    \"r--\",\n",
    "    alpha=0.75,\n",
    "    zorder=0,\n",
    ")\n",
    "energyUse_wDayOfWeek_LinearRegression.set_aspect(\"equal\")\n",
    "energyUse_wDayOfWeek_LinearRegression.set_xlim(lims)\n",
    "energyUse_wDayOfWeek_LinearRegression.set_ylim(lims)\n",
    "\n",
    "plt.xlabel(\"Actual Energy Use\")\n",
    "plt.ylabel(\"Predicted Energy Use\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(\n",
    "    LR_y_val, LR_y_val - LR_y_val_pred_dayOfWeek, s=25, cmap=plt.cm.coolwarm, zorder=10\n",
    ")\n",
    "\n",
    "xlims = ax.get_xlim()\n",
    "ax.plot(\n",
    "    xlims,\n",
    "    [\n",
    "        0.0,\n",
    "    ]\n",
    "    * 2,\n",
    "    \"k--\",\n",
    "    alpha=0.75,\n",
    "    zorder=0,\n",
    ")\n",
    "ax.set_xlim(xlims)\n",
    "\n",
    "plt.xlabel(\"Actual Energy Use\")\n",
    "plt.ylabel(\"Residual\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unscaled + dayOfWeek\n",
    "r2_lr = r2_score(LR_y_val, LR_y_val_pred_dayOfWeek_us)\n",
    "\n",
    "print(\n",
    "    \"The R^2 score for the linear regression model (unscaled + dayOfWeek) is: {:.3f}\".format(\n",
    "        r2_lr\n",
    "    )\n",
    ")\n",
    "\n",
    "MSE_lr = np.square(np.subtract(LR_y_val, LR_y_val_pred_dayOfWeek_us)).mean()\n",
    "RMSE_lr = math.sqrt(MSE_lr)\n",
    "\n",
    "print(\n",
    "    \"The RMSE score for the linear regression model (unscaled + dayOfWeek) is: {:.3f}\".format(\n",
    "        RMSE_lr\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_y_test_pred_dayOfWeek_us = model_us_lr_wDayOfWeek.predict(LR_X_test)\n",
    "# unscaled + dayOfWeek\n",
    "r2_lr = r2_score(LR_y_test, LR_y_test_pred_dayOfWeek_us)\n",
    "\n",
    "print(\n",
    "    \"The R^2 score for the linear regression model (unscaled + dayOfWeek) is: {:.3f}\".format(\n",
    "        r2_lr\n",
    "    )\n",
    ")\n",
    "\n",
    "MSE_lr = np.square(np.subtract(LR_y_test, LR_y_test_pred_dayOfWeek_us)).mean()\n",
    "RMSE_lr = math.sqrt(MSE_lr)\n",
    "\n",
    "print(\n",
    "    \"The RMSE score for the linear regression model (unscaled + dayOfWeek) is: {:.3f}\".format(\n",
    "        RMSE_lr\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot learning curve\n",
    "def plot_learning_curve(\n",
    "    train_loss, val_loss, train_metric, val_metric, metric_name=\"MeanAbsoluteError\"\n",
    "):\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_loss, \"r--\")\n",
    "    plt.plot(val_loss, \"b--\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend([\"train\", \"val\"], loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_metric, \"r--\")\n",
    "    plt.plot(val_metric, \"b--\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend([\"train\", \"val\"], loc=\"upper left\")\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "# function for residual plot\n",
    "def plot_residuals(model, val_y, y_pred):\n",
    "    residuals = val_y - y_pred\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(val_y, residuals, s=25, zorder=10)\n",
    "\n",
    "    ax.axhline(y=0, color=\"k\", linestyle=\"-\", linewidth=1, alpha=0.75, zorder=0)\n",
    "\n",
    "    plt.xlabel(\"Energy Usage\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(f\"Residuals Plot for {model}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    #function to crate scatter plot\n",
    "def scatter_plot(val_y, y_pred, model):\n",
    "    plt.scatter(val_y, y_pred)\n",
    "    plt.xlabel(\"Actual Energy Usage\")\n",
    "    plt.ylabel(\"Predicted Energy Usage\")\n",
    "    plt.title(f\"Actual vs Predicted Energy Usage for {model}\")\n",
    "    #red line\n",
    "    plt.plot([val_y.min(), val_y.max()], [val_y.min(), val_y.max()], \"r--\")\n",
    "    plt.show()\n",
    "\n",
    "    #function to calculate and print metrics\n",
    "def calculate_metrics(model, val_y, y_pred):\n",
    "    r2 = r2_score(val_y, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(val_y, y_pred))\n",
    "    mae = mean_absolute_error(val_y, y_pred)\n",
    "\n",
    "    print(f\"R2 Score for {model}: {r2}\")\n",
    "    print(f\"Root Mean Squared Error for {model}: {rmse}\")\n",
    "    print(f\"Mean Absolute Error for {model}: {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into traing, testing and validation\n",
    "\n",
    "with pd.option_context(\"mode.chained_assignment\", None):\n",
    "    train_data, test_data = train_test_split(\n",
    "        energyUse_df, test_size=0.3, shuffle=True, random_state=42\n",
    "    )\n",
    "    test_data, val_data = train_test_split(\n",
    "        test_data, test_size=0.5, shuffle=True, random_state=42\n",
    "    )\n",
    "\n",
    "# remove the target column from the data\n",
    "X_train = train_data.drop(columns=[\"TARGET_energy\", \"date\"])\n",
    "y_train = train_data[\"TARGET_energy\"]\n",
    "\n",
    "X_test = test_data.drop(columns=[\"TARGET_energy\", \"date\"])\n",
    "y_test = test_data[\"TARGET_energy\"]\n",
    "\n",
    "X_val = val_data.drop(columns=[\"TARGET_energy\", \"date\"])\n",
    "y_val = val_data[\"TARGET_energy\"]\n",
    "\n",
    "# train data - used to train the model\n",
    "# validation data - used to tune the hyperparameters\n",
    "# test data - used to evaluate the final model\n",
    "\n",
    "# print the shapes of the data\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"X_val shape: \", X_val.shape)\n",
    "print(\"y_val shape: \", y_val.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Neural Network on unchanged Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model values\n",
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_LAYER_DIM = 256  # this can be tuned later\n",
    "OUTPUT_CLASSES = 1\n",
    "\n",
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(HIDDEN_LAYER_DIM, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, activation=\"linear\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "\n",
    "# compile model\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mean_absolute_error\"]\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train, validation_data=(X_val, y_val), epochs=50, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curve\n",
    "plot_learning_curve(\n",
    "    history.history[\"loss\"],\n",
    "    history.history[\"val_loss\"],\n",
    "    history.history[\"mean_absolute_error\"],\n",
    "    history.history[\"val_mean_absolute_error\"],\n",
    ")\n",
    "#plot residual plot\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred = y_pred.flatten()\n",
    "plot_residuals(\"Base Model\", y_val, y_pred)\n",
    "\n",
    "#scatter plot\n",
    "scatter_plot(y_val, y_pred, \"Base Model\")\n",
    "\n",
    "#calculate metrics\n",
    "calculate_metrics(\"Base Model\", y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- the model performs extremely poorly\n",
    "- neds significant improvement\n",
    "  -over fitting is a major issue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2 = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(HIDDEN_LAYER_DIM, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model2.summary()\n",
    "tf.keras.utils.plot_model(model2, show_shapes=True)\n",
    "\n",
    "\n",
    "# compile model\n",
    "model2.compile(\n",
    "    optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mean_absolute_error\"]\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history2 = model2.fit(\n",
    "    X_train_scaled, y_train, validation_data=(X_val_scaled, y_val), epochs=50, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curve\n",
    "plot_learning_curve(\n",
    "    history2.history[\"loss\"],\n",
    "    history2.history[\"val_loss\"],\n",
    "    history2.history[\"mean_absolute_error\"],\n",
    "    history2.history[\"val_mean_absolute_error\"],\n",
    ")\n",
    "\n",
    "#plot residual plot\n",
    "y_pred = model2.predict(X_val_scaled)\n",
    "y_pred = y_pred.flatten()\n",
    "plot_residuals(\"Scaled Model\", y_val, y_pred)\n",
    "\n",
    "#scatter plot\n",
    "scatter_plot(y_val, y_pred, \"Scaled Model\")\n",
    "\n",
    "#calculate metrics\n",
    "calculate_metrics(\"Scaled Model\", y_val, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- still seems to be overfitting possibly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change batchsize\n",
    "\n",
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_LAYER_DIM = 256  # this can be tuned later\n",
    "OUTPUT_CLASSES = 1\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# create model\n",
    "model3 = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(HIDDEN_LAYER_DIM, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model3.compile(\n",
    "    optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mean_absolute_error\"]\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history3 = model3.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curve\n",
    "plot_learning_curve(\n",
    "    history3.history[\"loss\"],\n",
    "    history3.history[\"val_loss\"],\n",
    "    history3.history[\"mean_absolute_error\"],\n",
    "    history3.history[\"val_mean_absolute_error\"],\n",
    ")\n",
    "\n",
    "# plot residual plot\n",
    "y_pred = model3.predict(X_val_scaled)\n",
    "y_pred = y_pred.flatten()\n",
    "plot_residuals(\"Model 3\", y_val, y_pred)\n",
    "\n",
    "# scatter plot\n",
    "scatter_plot(y_val, y_pred, \"Model 3\")\n",
    "\n",
    "# calculate metrics\n",
    "calculate_metrics(\"Model 3\", y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try regularisation\n",
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_LAYER_DIM = 512  # this can be tuned later\n",
    "OUTPUT_CLASSES = 1\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# create model\n",
    "model4 = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l1(0.01),\n",
    "        ),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_initializer=\"he_normal\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model4.compile(\n",
    "    optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mean_absolute_error\"]\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history5 = model4.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curve\n",
    "plot_learning_curve(\n",
    "    history5.history[\"loss\"],\n",
    "    history5.history[\"val_loss\"],\n",
    "    history5.history[\"mean_absolute_error\"],\n",
    "    history5.history[\"val_mean_absolute_error\"],\n",
    ")\n",
    "\n",
    "# plot residual plot\n",
    "y_pred = model4.predict(X_val_scaled)\n",
    "y_pred = y_pred.flatten()\n",
    "plot_residuals(\"Model 4\", y_val, y_pred)\n",
    "\n",
    "# scatter plot\n",
    "scatter_plot(y_val, y_pred, \"Model 4\")\n",
    "\n",
    "# calculate metrics\n",
    "calculate_metrics(\"Model 4\", y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- still overfitting the model   \n",
    "-standard scaler is working better than minmax scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune regularisation\n",
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_LAYER_DIM = 512  # this can be tuned later\n",
    "OUTPUT_CLASSES = 1\n",
    "BATCH_SIZE = 128\n",
    "REGULARIZATIONFACTOR = 5\n",
    "\n",
    "# create model\n",
    "model5 = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_initializer=\"he_normal\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model5.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss='mean_squared_error',\n",
    "    metrics=[\"mean_absolute_error\", tf.keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history6 = model5.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curve\n",
    "plot_learning_curve(\n",
    "    history6.history[\"loss\"],\n",
    "    history6.history[\"val_loss\"],\n",
    "    history6.history[\"mean_absolute_error\"],\n",
    "    history6.history[\"val_mean_absolute_error\"],\n",
    ")\n",
    "\n",
    "# plot residual plot\n",
    "y_pred = model5.predict(X_val_scaled)\n",
    "y_pred = y_pred.flatten()\n",
    "plot_residuals(\"Model 5\", y_val, y_pred)\n",
    "\n",
    "# scatter plot\n",
    "scatter_plot(y_val, y_pred, \"Model 5\")\n",
    "\n",
    "# calculate metrics\n",
    "calculate_metrics(\"Model 5\", y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- not greatv still but no more overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 6 - Scaled Data, L2 Regularisation, DropOut and Batch Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tune regularisation\n",
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_LAYER_DIM = 256  # this can be tuned later\n",
    "OUTPUT_CLASSES = 1\n",
    "BATCH_SIZE = 64\n",
    "REGULARIZATIONFACTOR = 0.01\n",
    "\n",
    "# create model\n",
    "model6 = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(0.5),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_initializer=\"he_normal\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model6.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[\"mean_absolute_error\", tf.keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history7 = model6.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curve\n",
    "plot_learning_curve(\n",
    "    history7.history[\"loss\"],\n",
    "    history7.history[\"val_loss\"],\n",
    "    history7.history[\"mean_absolute_error\"],\n",
    "    history7.history[\"val_mean_absolute_error\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# residual plot\n",
    "y_pred = model6.predict(X_val_scaled)\n",
    "y_pred = y_pred.flatten()\n",
    "plot_residuals(\"Model 6\", y_val, y_pred)\n",
    "\n",
    "# scatter plot\n",
    "scatter_plot(y_val, y_pred, \"Model 6\")\n",
    "\n",
    "# calculate metrics\n",
    "calculate_metrics(\"Model 6\", y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- performing not well but better thasn the start\n",
    "- will try creatting features to represent the time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create time based features\n",
    "energyUse_df[\"date\"] = pd.to_datetime(energyUse_df[\"date\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "# create time-based features\n",
    "energyUse_df['hour'] = energyUse_df['date'].dt.hour\n",
    "energyUse_df['day'] = energyUse_df['date'].dt.day\n",
    "energyUse_df['month'] = energyUse_df['date'].dt.month\n",
    "energyUse_df['year'] = energyUse_df['date'].dt.year\n",
    "\n",
    "# drop rows with missing values\n",
    "energyUse_df = energyUse_df.dropna()\n",
    "\n",
    "# resplit the data\n",
    "train_data, test_data = train_test_split(energyUse_df, test_size=0.3, shuffle=True, random_state=42)\n",
    "test_data, val_data = train_test_split(test_data, test_size=0.5, shuffle=True, random_state=42)\n",
    "\n",
    "# remove the target column from the data\n",
    "X_train = train_data.drop(columns=[\"TARGET_energy\", \"date\"])\n",
    "y_train = train_data[\"TARGET_energy\"]\n",
    "\n",
    "X_test = test_data.drop(columns=[\"TARGET_energy\", \"date\"])\n",
    "y_test = test_data[\"TARGET_energy\"]\n",
    "\n",
    "X_val = val_data.drop(columns=[\"TARGET_energy\", \"date\"])\n",
    "y_val = val_data[\"TARGET_energy\"]\n",
    "\n",
    "# train data - used to train the model\n",
    "# validation data - used to tune the hyperparameters\n",
    "# test data - used to evaluate the final model\n",
    "\n",
    "# print the shapes of the data\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"X_val shape: \", X_val.shape)\n",
    "print(\"y_val shape: \", y_val.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_LAYER_DIM = 256  # this can be tuned later\n",
    "OUTPUT_CLASSES = 1\n",
    "BATCH_SIZE = 64\n",
    "REGULARIZATIONFACTOR = 0.01\n",
    "\n",
    "# create model\n",
    "model7 = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(0.5),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_initializer=\"he_normal\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model7.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[\"mean_absolute_error\", tf.keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history8 = model7.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#power transform data\n",
    "scaler = PowerTransformer(method='yeo-johnson').fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_val_transformed = scaler.transform(X_val)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_LAYER_DIM = 512  # this can be tuned later\n",
    "OUTPUT_CLASSES = 1\n",
    "BATCH_SIZE = 128\n",
    "REGULARIZATIONFACTOR = 0.05\n",
    "\n",
    "# create model\n",
    "model8 = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(0.5),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(0.5),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_initializer=\"he_normal\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model8.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[\"mean_absolute_error\", tf.keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history9 = model8.fit(\n",
    "    X_train_transformed,\n",
    "    y_train,\n",
    "    validation_data=(X_val_transformed, y_val),\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curve\n",
    "plot_learning_curve(\n",
    "    history9.history[\"loss\"],\n",
    "    history9.history[\"val_loss\"],\n",
    "    history9.history[\"mean_absolute_error\"],\n",
    "    history9.history[\"val_mean_absolute_error\"],\n",
    ")\n",
    "\n",
    "    #plot residual plot\n",
    "y_pred = model8.predict(X_val_transformed)\n",
    "y_pred = y_pred.flatten()\n",
    "plot_residuals(\"Model 8\", y_val, y_pred)\n",
    "\n",
    "#scatter plot\n",
    "scatter_plot(y_val, y_pred, \"Model 8\")\n",
    "\n",
    "#calculate metrics\n",
    "calculate_metrics(\"Model 8\", y_val, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_LAYER_DIM =    512\n",
    "OUTPUT_CLASSES = 1\n",
    "BATCH_SIZE = 256\n",
    "REGULARIZATIONFACTOR = 0.015\n",
    "DROPOUT = 0.5\n",
    "learningrate = 0.001\n",
    "optimizer = Adam(learning_rate=learningrate)\n",
    "\n",
    "# Define the callbacks\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=15)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.05, patience=10)\n",
    "\n",
    "# create model\n",
    "model9 = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_initializer=\"he_normal\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model9.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[\"mean_absolute_error\", tf.keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history10 = model9.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "moidel is overfitting - parameter tuning required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further tuning of model parameters\n",
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_LAYER_DIM = 512\n",
    "OUTPUT_CLASSES = 1\n",
    "BATCH_SIZE = 512\n",
    "REGULARIZATIONFACTOR = 0.0275\n",
    "DROPOUT = 0.55\n",
    "learningrate = 0.0015\n",
    "optimizer = Adam(learning_rate=learningrate)\n",
    "\n",
    "# Define the callbacks\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=15)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.05, patience=15)\n",
    "\n",
    "# create model\n",
    "model10 = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_initializer=\"he_normal\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model10.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[\"mean_absolute_error\", tf.keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history11 = model10.fit(\n",
    "    X_train_transformed,\n",
    "    y_train,\n",
    "    validation_data=(X_val_transformed, y_val),\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2 score\n",
    "# Generate predictions\n",
    "y_val_pred = model10.predict(X_val_transformed)\n",
    "# Flatten predictions to 1D array\n",
    "y_val_pred = y_val_pred.flatten()\n",
    "\n",
    "#residual plot\n",
    "plot_residuals(\"Model 10\", y_val, y_val_pred)\n",
    "\n",
    "#scatter plot\n",
    "scatter_plot(y_val, y_val_pred, \"Model 10\")\n",
    "\n",
    "#calculate metrics\n",
    "calculate_metrics(\"Model 10\", y_val, y_val_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rh6\n",
    "X_train= X_train.drop(columns=[\"RH_6\"])\n",
    "X_val = X_val.drop(columns=[\"RH_6\"])\n",
    "X_test = X_test.drop(columns=[\"RH_6\"])\n",
    "# power transform data\n",
    "scaler = PowerTransformer(method=\"yeo-johnson\").fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_val_transformed = scaler.transform(X_val)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "#standard scale data\n",
    "scaler = StandardScaler().fit(X_train_transformed)\n",
    "X_train_transformedandscaled = scaler.transform(X_train_transformed)\n",
    "X_val_transformedandscaled = scaler.transform(X_val_transformed)\n",
    "X_test_transformedandscaled = scaler.transform(X_test_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_LAYER_DIM = 512\n",
    "OUTPUT_CLASSES = 1\n",
    "BATCH_SIZE = 512\n",
    "REGULARIZATIONFACTOR = 0.0275\n",
    "DROPOUT = 0.55\n",
    "learningrate = 0.0015\n",
    "optimizer = Adam(learning_rate=learningrate)\n",
    "\n",
    "# Define the callbacks\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=15)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.05, patience=15)\n",
    "\n",
    "# create model\n",
    "model11 = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_initializer=\"he_normal\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model11.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[\"mean_absolute_error\", tf.keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history12 = model11.fit(\n",
    "    X_train_transformedandscaled,\n",
    "    y_train,\n",
    "    validation_data=(X_val_transformedandscaled, y_val),\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2 score\n",
    "# Generate predictions\n",
    "y_val_pred = model11.predict(X_val_transformed)\n",
    "# Flatten predictions to 1D array\n",
    "y_val_pred = y_val_pred.flatten()\n",
    "\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"R2 score: {r2}\")\n",
    "\n",
    "\n",
    "plt.scatter(y_val, y_val_pred)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs Predicted Values\")\n",
    "plt.plot(\n",
    "    [y_val.min(), y_val.max()], [y_val.min(), y_val.max()], color=\"red\"\n",
    ")  # y=x line\n",
    "plt.show()\n",
    "\n",
    "#residual plot\n",
    "plot_residuals(\"Model 11\", y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop t6\n",
    "X_train = X_train.drop(columns=[\"T6\"])\n",
    "X_val = X_val.drop(columns=[\"T6\"])\n",
    "X_test = X_test.drop(columns=[\"T6\"])\n",
    "# power transform data\n",
    "scaler = PowerTransformer(method=\"yeo-johnson\").fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_val_transformed = scaler.transform(X_val)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "# standard scale data\n",
    "scaler = StandardScaler().fit(X_train_transformed)\n",
    "X_train_transformedandscaled = scaler.transform(X_train_transformed)\n",
    "X_val_transformedandscaled = scaler.transform(X_val_transformed)\n",
    "X_test_transformedandscaled = scaler.transform(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_LAYER_DIM = 512\n",
    "OUTPUT_CLASSES = 1\n",
    "BATCH_SIZE = 512\n",
    "REGULARIZATIONFACTOR = 0.0275\n",
    "DROPOUT = 0.55\n",
    "learningrate = 0.0015\n",
    "optimizer = Adam(learning_rate=learningrate)\n",
    "\n",
    "# Define the callbacks\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=15)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.05, patience=15)\n",
    "\n",
    "# create model\n",
    "model12 = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_initializer=\"he_normal\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model12.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[\"mean_absolute_error\", tf.keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history13 = model12.fit(\n",
    "    X_train_transformedandscaled,\n",
    "    y_train,\n",
    "    validation_data=(X_val_transformedandscaled, y_val),\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2 score\n",
    "\n",
    "# Generate predictions\n",
    "y_val_pred = model12.predict(X_val_transformedandscaled)\n",
    "# Flatten predictions to 1D array\n",
    "y_val_pred = y_val_pred.flatten()\n",
    "\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "\n",
    "print(f\"R2 score: {r2}\")\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_val, y_val_pred)\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "# Calculate MSE (loss)\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "print(f\"MSE: {mse}\")\n",
    "\n",
    "\n",
    "plt.scatter(y_val, y_val_pred)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs Predicted Values\")\n",
    "plt.plot(\n",
    "    [y_val.min(), y_val.max()], [y_val.min(), y_val.max()], color=\"red\"\n",
    ")  # y=x line\n",
    "plt.show()\n",
    "\n",
    "# residual plot\n",
    "plot_residuals(\"Model 12\", y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column for number of seconbnds from midnight\n",
    "energyUse_df[\"date\"] = pd.to_datetime(energyUse_df[\"date\"])\n",
    "energyUse_df[\"seconds_from_midnight\"] = (\n",
    "    energyUse_df[\"date\"].dt.hour * 3600\n",
    "    + energyUse_df[\"date\"].dt.minute * 60\n",
    "    + energyUse_df[\"date\"].dt.second\n",
    ")\n",
    "#add column for day of the week\n",
    "energyUse_df[\"day_of_week\"] = energyUse_df[\"date\"].dt.dayofweek\n",
    "\n",
    "X_train = train_data.drop(columns=[\"TARGET_energy\", \"date\"])\n",
    "y_train = train_data[\"TARGET_energy\"]\n",
    "\n",
    "\n",
    "X_test = test_data.drop(columns=[\"TARGET_energy\", \"date\"])\n",
    "y_test = test_data[\"TARGET_energy\"]\n",
    "\n",
    "X_val = val_data.drop(columns=[\"TARGET_energy\", \"date\"])\n",
    "y_val = val_data[\"TARGET_energy\"]\n",
    "\n",
    "# power transform data\n",
    "scaler = PowerTransformer(method=\"yeo-johnson\").fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_val_transformed = scaler.transform(X_val)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "# standard scale data\n",
    "scaler = StandardScaler().fit(X_train_transformed)\n",
    "X_train_transformedandscaled = scaler.transform(X_train_transformed)\n",
    "X_val_transformedandscaled = scaler.transform(X_val_transformed)\n",
    "X_test_transformedandscaled = scaler.transform(X_test_transformed)\n",
    "\n",
    "# train data - used to train the model\n",
    "# validation data - used to tune the hyperparameters\n",
    "# test data - used to evaluate the final model\n",
    "\n",
    "# print the shapes of the data\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"X_val shape: \", X_val.shape)\n",
    "print(\"y_val shape: \", y_val.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_LAYER_DIM = 512\n",
    "OUTPUT_CLASSES = 1\n",
    "BATCH_SIZE = 512\n",
    "REGULARIZATIONFACTOR = 0.0275\n",
    "DROPOUT = 0.55\n",
    "learningrate = 0.0015\n",
    "optimizer = Adam(learning_rate=learningrate)\n",
    "\n",
    "# Define the callbacks\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=15)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.05, patience=15)\n",
    "\n",
    "# create model\n",
    "model13 = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_initializer=\"he_normal\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model13.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[\"mean_absolute_error\", tf.keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history14 = model13.fit(\n",
    "    X_train_transformedandscaled,\n",
    "    y_train,\n",
    "    validation_data=(X_val_transformedandscaled, y_val),\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    ")\n",
    "\n",
    "    # r2 score\n",
    "# Generate predictions\n",
    "y_val_pred = model13.predict(X_val_transformed)\n",
    "# Flatten predictions to 1D array\n",
    "y_val_pred = y_val_pred.flatten()\n",
    "\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"R2 score: {r2}\")\n",
    "\n",
    "\n",
    "plt.scatter(y_val, y_val_pred)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs Predicted Values\")\n",
    "plt.plot(\n",
    "    [y_val.min(), y_val.max()], [y_val.min(), y_val.max()], color=\"red\"\n",
    ")  # y=x line\n",
    "plt.show()\n",
    "\n",
    "# residual plot\n",
    "plot_residuals(\"Model 13\", y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop hour, day, month, year,\n",
    "X_train = X_train.drop(columns=[\"hour\", \"day\", \"month\", \"year\"])\n",
    "X_val = X_val.drop(columns=[\"hour\", \"day\", \"month\", \"year\"])\n",
    "X_test = X_test.drop(columns=[\"hour\", \"day\", \"month\", \"year\"])\n",
    "# power transform data\n",
    "scaler = PowerTransformer(method=\"yeo-johnson\").fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_val_transformed = scaler.transform(X_val)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "# standard scale data\n",
    "scaler = StandardScaler().fit(X_train_transformed)\n",
    "X_train_transformedandscaled = scaler.transform(X_train_transformed)\n",
    "X_val_transformedandscaled = scaler.transform(X_val_transformed)\n",
    "X_test_transformedandscaled = scaler.transform(X_test_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_LAYER_DIM = 512\n",
    "OUTPUT_CLASSES = 1\n",
    "BATCH_SIZE = 512\n",
    "REGULARIZATIONFACTOR = 0.0275\n",
    "DROPOUT = 0.55\n",
    "learningrate = 0.0015\n",
    "optimizer = Adam(learning_rate=learningrate)\n",
    "\n",
    "# Define the callbacks\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=15)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.05, patience=15)\n",
    "\n",
    "# create model\n",
    "model14 = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_initializer=\"he_normal\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model14.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[\"mean_absolute_error\", tf.keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history15 = model14.fit(\n",
    "    X_train_transformedandscaled,\n",
    "    y_train,\n",
    "    validation_data=(X_val_transformedandscaled, y_val),\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    ")\n",
    "\n",
    "# r2 score\n",
    "# Generate predictions\n",
    "y_val_pred = model14.predict(X_val_transformed)\n",
    "# Flatten predictions to 1D array\n",
    "y_val_pred = y_val_pred.flatten()\n",
    "\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"R2 score: {r2}\")\n",
    "\n",
    "\n",
    "plt.scatter(y_val, y_val_pred)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs Predicted Values\")\n",
    "plt.plot(\n",
    "    [y_val.min(), y_val.max()], [y_val.min(), y_val.max()], color=\"red\"\n",
    ")  # y=x line\n",
    "plt.show()\n",
    "\n",
    "# residual plot\n",
    "plot_residuals(\"Model 14\", y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add colkumn to check if day is weekday or weekend\n",
    "X_train[\"is_weekend\"] = X_train[\"day_of_week\"].apply(lambda x: 1 if x >= 5 else 0)\n",
    "X_val[\"is_weekend\"] = X_val[\"day_of_week\"].apply(lambda x: 1 if x >= 5 else 0)\n",
    "X_test[\"is_weekend\"] = X_test[\"day_of_week\"].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# power transform data\n",
    "scaler = PowerTransformer(method=\"yeo-johnson\").fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_val_transformed = scaler.transform(X_val)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "# standard scale data\n",
    "scaler = StandardScaler().fit(X_train_transformed)\n",
    "X_train_transformedandscaled = scaler.transform(X_train_transformed)\n",
    "X_val_transformedandscaled = scaler.transform(X_val_transformed)\n",
    "X_test_transformedandscaled = scaler.transform(X_test_transformed)\n",
    "\n",
    "#shape of the data\n",
    "print(\"X_train shape: \", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_LAYER_DIM = 512\n",
    "OUTPUT_CLASSES = 1\n",
    "BATCH_SIZE = 512\n",
    "REGULARIZATIONFACTOR = 0.0275\n",
    "DROPOUT = 0.55\n",
    "learningrate = 0.0015\n",
    "optimizer = Adam(learning_rate=learningrate)\n",
    "\n",
    "# Define the callbacks\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=15)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.05, patience=15)\n",
    "\n",
    "# create model\n",
    "model15 = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_initializer=\"he_normal\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model15.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[\"mean_absolute_error\", tf.keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history16 = model15.fit(\n",
    "    X_train_transformedandscaled,\n",
    "    y_train,\n",
    "    validation_data=(X_val_transformedandscaled, y_val),\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    ")\n",
    "\n",
    "# r2 score\n",
    "# Generate predictions\n",
    "y_val_pred = model15.predict(X_val_transformedandscaled)\n",
    "# Flatten predictions to 1D array\n",
    "y_val_pred = y_val_pred.flatten()\n",
    "\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"R2 score: {r2}\")\n",
    "\n",
    "\n",
    "plt.scatter(y_val, y_val_pred)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs Predicted Values\")\n",
    "plt.plot(\n",
    "    [y_val.min(), y_val.max()], [y_val.min(), y_val.max()], color=\"red\"\n",
    ")  # y=x line\n",
    "plt.show()\n",
    "\n",
    "# residual plot\n",
    "plot_residuals(\"Model 15\", y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get metrics\n",
    "calculate_metrics(\"Model 15\", y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change date column to datetime\n",
    "energyUse_df[\"date\"] = pd.to_datetime(energyUse_df[\"date\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# add seconds from midnight column\n",
    "energyUse_df[\"seconds_from_midnight\"] = (\n",
    "    energyUse_df[\"date\"].dt.hour * 3600\n",
    "    + energyUse_df[\"date\"].dt.minute * 60\n",
    "    + energyUse_df[\"date\"].dt.second\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# add day_of_week column\n",
    "energyUse_df[\"day_of_week\"] = energyUse_df[\"date\"].dt.dayofweek\n",
    "\n",
    "# add is_weekend column\n",
    "energyUse_df[\"is_weekend\"] = energyUse_df[\"day_of_week\"].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# describe the data and print the first 5 rows\n",
    "energyUse_df.describe()\n",
    "energyUse_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop rows with missing values\n",
    "\n",
    "energyUse_df = energyUse_df.dropna()\n",
    "\n",
    "\n",
    "# split the data\n",
    "\n",
    "train_data, test_data = train_test_split(energyUse_df, test_size=0.3, shuffle=True, random_state=42)\n",
    "\n",
    "test_data, val_data = train_test_split(test_data, test_size=0.5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# rebalance dataset by maually duplicating the instances of higher values\n",
    "\n",
    "# Find the number of instances where the target is greater than 100\n",
    "\n",
    "high_values = train_data[train_data[\"TARGET_energy\"] > 100]\n",
    "\n",
    "print(\"Number of instances with target greater than 100: \", high_values.shape[0])\n",
    "\n",
    "\n",
    "# upsample the high values\n",
    "\n",
    "train_data = pd.concat([train_data, high_values], ignore_index=True)\n",
    "\n",
    "test_data = pd.concat([test_data, high_values], ignore_index=True)\n",
    "\n",
    "val_data = pd.concat([val_data, high_values], ignore_index=True)\n",
    "\n",
    "\n",
    "# remove the target column from the data\n",
    "\n",
    "X_train = train_data.drop(columns=[\"TARGET_energy\", \"date\"])\n",
    "\n",
    "y_train = train_data[\"TARGET_energy\"]\n",
    "\n",
    "X_val = val_data.drop(columns=[\"TARGET_energy\", \"date\"])\n",
    "\n",
    "y_val = val_data[\"TARGET_energy\"]\n",
    "\n",
    "X_test = test_data.drop(columns=[\"TARGET_energy\", \"date\"])\n",
    "\n",
    "y_test = test_data[\"TARGET_energy\"]\n",
    "\n",
    "# # drop random variable columns\n",
    "X_train = X_train.drop(columns=[\"rv1\", \"rv2\"])\n",
    "X_val = X_val.drop(columns=[\"rv1\", \"rv2\"])\n",
    "X_test = X_test.drop(columns=[\"rv1\", \"rv2\"])\n",
    "\n",
    "#drop rh6 and t6\n",
    "X_train = X_train.drop(columns=[\"RH_6\", \"T6\"])\n",
    "X_val = X_val.drop(columns=[\"RH_6\", \"T6\"])\n",
    "X_test = X_test.drop(columns=[\"RH_6\", \"T6\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard scale the data and power transform the data\n",
    "scaler = PowerTransformer(method=\"yeo-johnson\").fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_val_transformed = scaler.transform(X_val)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "scaler = StandardScaler().fit(X_train_transformed)\n",
    "X_train_transformedandscaled = scaler.transform(X_train_transformed)\n",
    "X_val_transformedandscaled = scaler.transform(X_val_transformed)\n",
    "X_test_transformedandscaled = scaler.transform(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted loss function\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    y_true_float = tf.cast(y_true, tf.float32)\n",
    "    weights = tf.sqrt(y_true_float) + 1\n",
    "    return tf.reduce_sum(weights * tf.square(y_true_float - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy of models - so I don't have to keep scrolling\n",
    "# model ensemble 1\n",
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_LAYER_DIM = 512\n",
    "OUTPUT_CLASSES = 1\n",
    "BATCH_SIZE = 256\n",
    "REGULARIZATIONFACTOR = 0.03\n",
    "DROPOUT = 0.50\n",
    "learningrate = 0.002\n",
    "DELTA = 1.0\n",
    "optimizer = Adam(learning_rate=learningrate)\n",
    "\n",
    "\n",
    "\n",
    "# Define the callbacks\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=20)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.05, patience=20)\n",
    "\n",
    "\n",
    "# create model\n",
    "model_ensemble_1 = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_initializer=\"he_normal\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model_ensemble_1.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=weighted_loss,\n",
    "    metrics=[\"mean_absolute_error\", tf.keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history_ensemble_1 = model_ensemble_1.fit(\n",
    "    X_train_transformedandscaled,\n",
    "    y_train,\n",
    "    # sample_weight=sample_weights,\n",
    "    validation_data=(X_val_transformedandscaled, y_val),\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "y_val_pred = model_ensemble_1.predict(X_val_transformedandscaled)\n",
    "# flatten predictions\n",
    "y_val_pred = y_val_pred.flatten()\n",
    "\n",
    "plot_residuals(\"Model Ensemble 1\", y_val, y_val_pred)\n",
    "\n",
    "# get scores\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "print(f\"R2 score: {r2}\")\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_val, y_val_pred)\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 15\n",
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_LAYER_DIM = 512\n",
    "OUTPUT_CLASSES = 1\n",
    "BATCH_SIZE = 256\n",
    "REGULARIZATIONFACTOR = 0.035\n",
    "DROPOUT = 0.50\n",
    "learningrate = 0.0015\n",
    "optimizer = Adam(learning_rate=learningrate)\n",
    "\n",
    "# Define the callbacks\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=15)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.07, patience=15)\n",
    "\n",
    "# create model\n",
    "model15 = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_initializer=\"he_normal\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model15.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=Huber(),\n",
    "    metrics=[\"mean_absolute_error\", tf.keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history16 = model15.fit(\n",
    "    X_train_transformedandscaled,\n",
    "    y_train,\n",
    "    validation_data=(X_val_transformedandscaled, y_val),\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residual plot\n",
    "# get predictions\n",
    "y_val_pred = model15.predict(X_val_transformedandscaled)\n",
    "# flatten predictions\n",
    "y_val_pred = y_val_pred.flatten()\n",
    "plot_residuals(\"Model 15\", y_val, y_val_pred)\n",
    "\n",
    "# cakuculate metrics\n",
    "calculate_metrics(\"Model 15\", y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 16\n",
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_LAYER_DIM = 512\n",
    "OUTPUT_CLASSES = 1\n",
    "BATCH_SIZE = 256\n",
    "REGULARIZATIONFACTOR = 0.03\n",
    "DROPOUT = 0.50\n",
    "learningrate = 0.0015\n",
    "optimizer = Adam(learning_rate=learningrate)\n",
    "\n",
    "# Define the callbacks\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=15)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.07, patience=15)\n",
    "\n",
    "# create model\n",
    "model16 = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_initializer=\"he_normal\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model16.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=weighted_loss,\n",
    "    metrics=[\"mean_absolute_error\", tf.keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history17 = model16.fit(\n",
    "    X_train_transformedandscaled,\n",
    "    y_train,\n",
    "    validation_data=(X_val_transformedandscaled, y_val),\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residual plot\n",
    "# get predictions\n",
    "y_val_pred = model16.predict(X_val_transformedandscaled)\n",
    "# flatten predictions\n",
    "y_val_pred = y_val_pred.flatten()\n",
    "plot_residuals(\"Model 16\", y_val, y_val_pred)\n",
    "\n",
    "# cakuculate metrics\n",
    "calculate_metrics(\"Model 16\", y_val, y_val_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 21 - decision tree regressor\n",
    "model21 = DecisionTreeRegressor(max_depth=30, min_samples_split=10, min_samples_leaf=10)\n",
    "\n",
    "#weight is cube of y\n",
    "weights = np.power(y_train, 2)\n",
    "\n",
    "model21.fit(X_train_transformedandscaled, y_train, sample_weight=weights)\n",
    "\n",
    "\n",
    "#plot residuals\n",
    "plot_residuals(\"Model 21\", y_val, y_val_pred)\n",
    "\n",
    "#calculate metrics\n",
    "calculate_metrics(\"Model 21\", y_val, y_val_pred)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 22 - neural network but ensemble\n",
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_LAYER_DIM = 512\n",
    "OUTPUT_CLASSES = 1\n",
    "BATCH_SIZE = 256\n",
    "REGULARIZATIONFACTOR = 0.03\n",
    "DROPOUT = 0.50\n",
    "learningrate = 0.002\n",
    "DELTA = 1.0\n",
    "optimizer = Adam(learning_rate=learningrate)\n",
    "\n",
    "# get values over 100\n",
    "threshold = 80\n",
    "\n",
    "# Split the training data into two subsets\n",
    "X_train_low = X_train_transformedandscaled[y_train <= threshold]\n",
    "y_train_low = y_train[y_train <= threshold]\n",
    "X_train_high = X_train_transformedandscaled[y_train > threshold]\n",
    "y_train_high = y_train[y_train > threshold]\n",
    "# Split the validation data into two subsets\n",
    "# Split the validation data into two subsets\n",
    "X_val_low = X_val_transformedandscaled[y_val <= threshold]\n",
    "y_val_low = y_val[y_val <= threshold]\n",
    "X_val_high = X_val_transformedandscaled[y_val > threshold]\n",
    "y_val_high = y_val[y_val > threshold]\n",
    "\n",
    "# Define the callbacks\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=15)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.05, patience=15)\n",
    "\n",
    "\n",
    "# create model_low_values\n",
    "model_low_values = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_initializer=\"he_normal\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model_low_values.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=weighted_loss,\n",
    "    metrics=[\"mean_absolute_error\", tf.keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history_low_values = model_low_values.fit(\n",
    "    X_train_low,\n",
    "    y_train_low,\n",
    "    # sample_weight=sample_weights,\n",
    "    validation_data=(X_val_low, y_val_low),\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 22 - neural network but ensemble\n",
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_LAYER_DIM = 512\n",
    "OUTPUT_CLASSES = 1\n",
    "BATCH_SIZE = 256\n",
    "REGULARIZATIONFACTOR = 0.03\n",
    "DROPOUT = 0.50\n",
    "learningrate = 0.002\n",
    "DELTA = 1.0\n",
    "optimizer = Adam(learning_rate=learningrate)\n",
    "# create model_high_values\n",
    "model_high_values = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_initializer=\"he_normal\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model_high_values.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=weighted_loss,\n",
    "    metrics=[\"mean_absolute_error\", tf.keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history_high_values = model_high_values.fit(\n",
    "    X_train_high,\n",
    "    y_train_high,\n",
    "    # sample_weight=sample_weights,\n",
    "    validation_data=(X_val_high, y_val_high),\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_val_low.shape)\n",
    "print(X_val_high.shape)\n",
    "\n",
    "# Use the appropriate model to make predictions for each subset\n",
    "y_val_pred_low = model_low_values.predict(X_val_low).flatten()\n",
    "y_val_pred_high = model_high_values.predict(X_val_high).flatten()\n",
    "\n",
    "# Concatenate the predictions and the true values\n",
    "y_val_pred = np.concatenate([y_val_pred_low, y_val_pred_high])\n",
    "y_val_true = np.concatenate([y_val[y_val <= threshold], y_val[y_val > threshold]])\n",
    "\n",
    "# Flatten the predictions\n",
    "y_val_pred = y_val_pred.flatten()\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_val_true, y_val_pred)\n",
    "print(f\"R2 score: {r2}\")\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "# plot residuals\n",
    "plot_residuals(\"Model 22\", y_val_true, y_val_pred)\n",
    "\n",
    "# scatter plot of actual vs predicted values\n",
    "plt.scatter(y_val_true, y_val_pred)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs Predicted Values\")\n",
    "plt.plot(\n",
    "    [y_val_true.min(), y_val_true.max()], [y_val_true.min(), y_val_true.max()], color=\"red\"\n",
    ")  # y=x line\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of models\n",
    "models = [model16, model15, model14, model21]\n",
    "\n",
    "\n",
    "# stacking models\n",
    "HIDDEN_LAYER_DIM = 512\n",
    "OUTPUT_CLASSES = 1\n",
    "BATCH_SIZE = 256\n",
    "learningrate = 0.0015\n",
    "optimizer = Adam(learning_rate=learningrate)\n",
    "\n",
    "# create meta features\n",
    "# create meta features\n",
    "meta_features = np.column_stack(\n",
    "    [model.predict(X_train_transformedandscaled).reshape(-1, 1) for model in models] +\n",
    "     [model_low_values.predict(X_train_transformedandscaled).reshape(-1, 1), model_high_values.predict(X_train_transformedandscaled).reshape(-1, 1)]\n",
    "\n",
    ")\n",
    "\n",
    "# create meta features for validation data\n",
    "meta_val_features = np.column_stack(\n",
    "    [model.predict(X_val_transformedandscaled).reshape(-1, 1) for model in models]+\n",
    "    [\n",
    "        model_low_values.predict(X_val_transformedandscaled).reshape(-1, 1),\n",
    "        model_high_values.predict(X_val_transformedandscaled).reshape(-1, 1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "meta_model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=6),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_regularizer=regularizers.l2(0.5),  # L2 regularization\n",
    "        ),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_regularizer=regularizers.l2(0.5),  # L2 regularization\n",
    "        ),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_regularizer=regularizers.l2(0.5),  # L2 regularization\n",
    "        ),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_initializer=\"he_normal\"),\n",
    "    ]\n",
    ")\n",
    "# Compile the meta-model\n",
    "meta_model.compile(\n",
    "    loss=weighted_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\"mean_absolute_error\", tf.keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history_meta = meta_model.fit(\n",
    "    meta_features,\n",
    "    y_train,\n",
    "    validation_data=(meta_val_features, y_val),\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "\n",
    "# Make final predictions\n",
    "final_predictions = meta_model.predict(meta_val_features)\n",
    "\n",
    "# Calculate and print scores\n",
    "r2 = r2_score(y_val, final_predictions)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, final_predictions))\n",
    "mae = mean_absolute_error(y_val, final_predictions)\n",
    "print(f\"R2 score: {r2}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "# Create scatter and residual plots\n",
    "plt.scatter(y_val, final_predictions)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs Predicted Values\")\n",
    "plt.plot(\n",
    "    [y_val.min(), y_val.max()], [y_val.min(), y_val.max()], color=\"red\"\n",
    ")  # y=x line\n",
    "plt.show()\n",
    "\n",
    "# flatten predictions\n",
    "final_predictions_flat = final_predictions.flatten()\n",
    "# Plot residuals\n",
    "plot_residuals(\"Model Ensemble\", y_val, final_predictions_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted loss function - copy\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    y_true_float = tf.cast(y_true, tf.float32)\n",
    "    weights = tf.sqrt(y_true_float) + 1\n",
    "    return tf.reduce_sum(weights * tf.square(y_true_float - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model - model 22\n",
    "\n",
    "# model 22 - neural network but ensemble\n",
    "INPUT_DIM = X_train.shape[1]\n",
    "HIDDEN_LAYER_DIM = 512\n",
    "OUTPUT_CLASSES = 1\n",
    "BATCH_SIZE = 256\n",
    "REGULARIZATIONFACTOR = 0.03\n",
    "DROPOUT = 0.50\n",
    "learningrate = 0.002\n",
    "DELTA = 1.0\n",
    "optimizer = Adam(learning_rate=learningrate)\n",
    "\n",
    "\n",
    "\n",
    "# get values over 100\n",
    "threshold = 100\n",
    "\n",
    "\n",
    "\n",
    "# Split the training data into two subsets\n",
    "X_train_low = X_train_transformedandscaled[y_train <= threshold]\n",
    "y_train_low = y_train[y_train <= threshold]\n",
    "X_train_high = X_train_transformedandscaled[y_train > threshold]\n",
    "y_train_high = y_train[y_train > threshold]\n",
    "\n",
    "# Split the validation data into two subsets\n",
    "X_val_low = X_val_transformedandscaled[y_val <= threshold]\n",
    "y_val_low = y_val[y_val <= threshold]\n",
    "X_val_high = X_val_transformedandscaled[y_val > threshold]\n",
    "y_val_high = y_val[y_val > threshold]\n",
    "\n",
    "# Define the callbacks\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=15)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.05, patience=15)\n",
    "\n",
    "\n",
    "# create model_low_values\n",
    "model_low_values = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_initializer=\"he_normal\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model_low_values.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=weighted_loss,\n",
    "    metrics=[\"mean_absolute_error\", tf.keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history_low_values = model_low_values.fit(\n",
    "    X_train_low,\n",
    "    y_train_low,\n",
    "    validation_data=(X_val_low, y_val_low),\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    ")\n",
    "\n",
    "# create model_high_values\n",
    "model_high_values = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(INPUT_DIM)),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            HIDDEN_LAYER_DIM,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            kernel_regularizer=regularizers.l2(REGULARIZATIONFACTOR),\n",
    "        ),\n",
    "        Dropout(DROPOUT),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dense(OUTPUT_CLASSES, kernel_initializer=\"he_normal\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model_high_values.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=weighted_loss,\n",
    "    metrics=[\"mean_absolute_error\", tf.keras.metrics.RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history_high_values = model_high_values.fit(\n",
    "    X_train_high,\n",
    "    y_train_high,\n",
    "    validation_data=(X_val_high, y_val_high),\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    ")\n",
    "\n",
    "# Use the appropriate model to make predictions for each subset\n",
    "y_val_pred_low = model_low_values.predict(X_val_low).flatten()\n",
    "y_val_pred_high = model_high_values.predict(X_val_high).flatten()\n",
    "\n",
    "# Concatenate the predictions and the true values\n",
    "y_val_pred = np.concatenate([y_val_pred_low, y_val_pred_high])\n",
    "y_val_true = np.concatenate([y_val[y_val <= threshold], y_val[y_val > threshold]])\n",
    "\n",
    "# Flatten the predictions\n",
    "y_val_pred = y_val_pred.flatten()\n",
    "# calcul;ate metrics\n",
    "calculate_metrics(\"Model 22\", y_val_true, y_val_pred)\n",
    "\n",
    "# plot residuals\n",
    "plot_residuals(\"Model 22\", y_val_true, y_val_pred)\n",
    "\n",
    "# scatter plot of actual vs predicted values\n",
    "scatter_plot(y_val_true, y_val_pred, \"Best Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model on test data\n",
    "# split test data\n",
    "X_test_low = X_test_transformedandscaled[y_test <= threshold]\n",
    "y_test_low = y_test[y_test <= threshold]\n",
    "X_test_high = X_test_transformedandscaled[y_test > threshold]\n",
    "y_test_high = y_test[y_test > threshold]\n",
    "\n",
    "# Use the appropriate model to make predictions for each subset\n",
    "y_test_pred_low = model_low_values.predict(X_test_low).flatten()\n",
    "y_test_pred_high = model_high_values.predict(X_test_high).flatten()\n",
    "\n",
    "# Concatenate the predictions and the true values\n",
    "y_test_pred = np.concatenate([y_test_pred_low, y_test_pred_high])\n",
    "y_test_true = np.concatenate([y_test[y_test <= threshold], y_test[y_test > threshold]])\n",
    "\n",
    "# Flatten the predictions\n",
    "y_test_pred = y_test_pred.flatten()\n",
    "# calculate metrics\n",
    "calculate_metrics(\"Model 22\", y_test_true, y_test_pred)\n",
    "\n",
    "# plot residuals\n",
    "plot_residuals(\"Best Model on Test Data\", y_test_true, y_test_pred)\n",
    "\n",
    "# scatter plot of actual vs predicted values\n",
    "scatter_plot( y_test_true, y_test_pred, \"Best Model - Test Data\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
